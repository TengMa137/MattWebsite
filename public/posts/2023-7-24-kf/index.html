<!DOCTYPE html>
<html lang="en">

<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
  <title>
  Kalman Filter explained with derivation · Welcome
</title>
  <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="color-scheme" content="light dark">




<meta name="author" content="Matt">
<meta name="description" content="Kalman Filter (KF) is a recursive state estimator widely used in sensor/data fusion. It&rsquo;s an optimal Bayesian filter in linear Gaussian system, since it minimizes the estimated error covariance. It is also known as Gaussian Filter. KF is not applicable to discrete or hybrid state spaces. This blog helps understand Kalman Filter with simple examples and presents mathematical derivation from two different perspectives.

  Modeling Robot State Estimation
  
    
    Link to heading
  

CLICK ME
Now we want to estimate the state of a robot &lt;$p, v$&gt;, $p = [p_x, p_y, p_z]^T$ for position and $v = [v_x, v_y, v_z]^T$ for velocity. The state at time $k-1$ is $x_{k-1}=(p_{k-1},v_{k-1})$, we can predict the state at time $k$ if the accelaration $a_k$ is given:
$$
p_k = p_{k-1}&#43;v_{k-1} \Delta t &#43; \frac{1}{2}a_k (\Delta t)^2 \
v_k = v_{k-1}&#43;a_k \Delta t
$$
In matrix form:
$$
x_k = Ax_{k-1}&#43;Ba_k
$$
where $A$ is state transition matrix and $B$ is called control input matrix.
$$
A = \begin{bmatrix}1 &amp; \Delta t \ 0 &amp; 1 \end{bmatrix},
B = \begin{bmatrix} \frac{1}{2}(\Delta t)^2 \ \Delta t \end{bmatrix}
$$
This is the ideal case. In reality the robot is disturbed by so-called process noise, which models the uncertainty introduced by the state transition. Usually we assume the process noise $\omega_k$ follows a Gaussian distribution (it has to be Gaussian in KF) with zero mean $\omega_k \sim N(0,Q_k)$, $Q_k$ is the covariance matrix. Another observation of the state $z_k$ comes from sensor such as GPS, with measurement noise in Gaussian $v_k \sim N(0, R_k)$. State space model:


$$
\begin{aligned}
&x_k = Ax_{k-1}&#43;Ba_k&#43;\omega_k \\
&z_k = Hx_k&#43;v_k
\end{aligned}
$$



where $H_k$ is measurement matrix that map the state space into the observation space.">
<meta name="keywords" content="blog,developer,personal">



  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="Kalman Filter explained with derivation">
  <meta name="twitter:description" content="Kalman Filter (KF) is a recursive state estimator widely used in sensor/data fusion. It’s an optimal Bayesian filter in linear Gaussian system, since it minimizes the estimated error covariance. It is also known as Gaussian Filter. KF is not applicable to discrete or hybrid state spaces. This blog helps understand Kalman Filter with simple examples and presents mathematical derivation from two different perspectives.
Modeling Robot State Estimation Link to heading CLICK MENow we want to estimate the state of a robot &lt;$p, v$&gt;, $p = [p_x, p_y, p_z]^T$ for position and $v = [v_x, v_y, v_z]^T$ for velocity. The state at time $k-1$ is $x_{k-1}=(p_{k-1},v_{k-1})$, we can predict the state at time $k$ if the accelaration $a_k$ is given: $$ p_k = p_{k-1}&#43;v_{k-1} \Delta t &#43; \frac{1}{2}a_k (\Delta t)^2 \ v_k = v_{k-1}&#43;a_k \Delta t $$ In matrix form: $$ x_k = Ax_{k-1}&#43;Ba_k $$ where $A$ is state transition matrix and $B$ is called control input matrix. $$ A = \begin{bmatrix}1 &amp; \Delta t \ 0 &amp; 1 \end{bmatrix}, B = \begin{bmatrix} \frac{1}{2}(\Delta t)^2 \ \Delta t \end{bmatrix} $$ This is the ideal case. In reality the robot is disturbed by so-called process noise, which models the uncertainty introduced by the state transition. Usually we assume the process noise $\omega_k$ follows a Gaussian distribution (it has to be Gaussian in KF) with zero mean $\omega_k \sim N(0,Q_k)$, $Q_k$ is the covariance matrix. Another observation of the state $z_k$ comes from sensor such as GPS, with measurement noise in Gaussian $v_k \sim N(0, R_k)$. State space model: $$\begin{aligned}&amp;x_k = Ax_{k-1}&#43;Ba_k&#43;\omega_k \\&amp;z_k = Hx_k&#43;v_k\end{aligned}$$where $H_k$ is measurement matrix that map the state space into the observation space.">

<meta property="og:url" content="http://localhost:1313/posts/2023-7-24-kf/">
  <meta property="og:site_name" content="Welcome">
  <meta property="og:title" content="Kalman Filter explained with derivation">
  <meta property="og:description" content="Kalman Filter (KF) is a recursive state estimator widely used in sensor/data fusion. It’s an optimal Bayesian filter in linear Gaussian system, since it minimizes the estimated error covariance. It is also known as Gaussian Filter. KF is not applicable to discrete or hybrid state spaces. This blog helps understand Kalman Filter with simple examples and presents mathematical derivation from two different perspectives.
Modeling Robot State Estimation Link to heading CLICK MENow we want to estimate the state of a robot &lt;$p, v$&gt;, $p = [p_x, p_y, p_z]^T$ for position and $v = [v_x, v_y, v_z]^T$ for velocity. The state at time $k-1$ is $x_{k-1}=(p_{k-1},v_{k-1})$, we can predict the state at time $k$ if the accelaration $a_k$ is given: $$ p_k = p_{k-1}&#43;v_{k-1} \Delta t &#43; \frac{1}{2}a_k (\Delta t)^2 \ v_k = v_{k-1}&#43;a_k \Delta t $$ In matrix form: $$ x_k = Ax_{k-1}&#43;Ba_k $$ where $A$ is state transition matrix and $B$ is called control input matrix. $$ A = \begin{bmatrix}1 &amp; \Delta t \ 0 &amp; 1 \end{bmatrix}, B = \begin{bmatrix} \frac{1}{2}(\Delta t)^2 \ \Delta t \end{bmatrix} $$ This is the ideal case. In reality the robot is disturbed by so-called process noise, which models the uncertainty introduced by the state transition. Usually we assume the process noise $\omega_k$ follows a Gaussian distribution (it has to be Gaussian in KF) with zero mean $\omega_k \sim N(0,Q_k)$, $Q_k$ is the covariance matrix. Another observation of the state $z_k$ comes from sensor such as GPS, with measurement noise in Gaussian $v_k \sim N(0, R_k)$. State space model: $$\begin{aligned}&amp;x_k = Ax_{k-1}&#43;Ba_k&#43;\omega_k \\&amp;z_k = Hx_k&#43;v_k\end{aligned}$$where $H_k$ is measurement matrix that map the state space into the observation space.">
  <meta property="og:locale" content="en">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2023-07-24T00:00:00+00:00">
    <meta property="article:modified_time" content="2023-07-24T00:00:00+00:00">




<link rel="canonical" href="http://localhost:1313/posts/2023-7-24-kf/">



<link rel="preload" href="/fonts/fa-brands-400.woff2" as="font" type="font/woff2" crossorigin>
<link rel="preload" href="/fonts/fa-regular-400.woff2" as="font" type="font/woff2" crossorigin>
<link rel="preload" href="/fonts/fa-solid-900.woff2" as="font" type="font/woff2" crossorigin>


  
  
  <link rel="stylesheet" href="/css/coder.css" media="screen">






  
    
    
    <link rel="stylesheet" href="/css/coder-dark.css" media="screen">
  



 




<link rel="icon" type="image/svg+xml" href="/images/favicon.svg" sizes="any">
<link rel="icon" type="image/png" href="/images/favicon-32x32.png" sizes="32x32">
<link rel="icon" type="image/png" href="/images/favicon-16x16.png" sizes="16x16">

<link rel="apple-touch-icon" href="/images/apple-touch-icon.png">
<link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png">

<link rel="manifest" href="/site.webmanifest">
<link rel="mask-icon" href="/images/safari-pinned-tab.svg" color="#5bbad5">










<script>
MathJax = {
  tex: {
    inlineMath: [['$', '$'], ['\\(', '\\)']],
    displayMath: [['$$', '$$'], ['\\[', '\\]']],
    processEscapes: true,
    processEnvironments: true,
    packages: {'[+]': ['ams', 'noerrors']}
  },
  svg: {
    fontCache: 'global'
  },
  options: {
    renderActions: {
       
      find_script_mathtex: [10, function (doc) {
        for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
          const display = !!node.type.match(/; *mode=display/);
          const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
          const text = document.createTextNode('');
          node.parentNode.replaceChild(text, node);
          math.start = {node: text, delim: '', n: 0};
          math.end = {node: text, delim: '', n: 0};
          doc.math.push(math);
        }
      }, '']
    }
  }
};
</script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>


</head>






<body class="preload-transitions colorscheme-auto">
  
<div class="float-container">
    <a id="dark-mode-toggle" class="colorscheme-toggle">
        <i class="fa-solid fa-adjust fa-fw" aria-hidden="true"></i>
    </a>
</div>


  <main class="wrapper">
    <nav class="navigation">
  <section class="container">
    
    <a class="navigation-title" href="http://localhost:1313/">
      Welcome
    </a>
    
    
      <input type="checkbox" id="menu-toggle" />
      <label class="menu-button float-right" for="menu-toggle">
        <i class="fa-solid fa-bars fa-fw" aria-hidden="true"></i>
      </label>
      <ul class="navigation-list">
        
          
            <li class="navigation-item">
              <a class="navigation-link " href="/about/">About</a>
            </li>
          
            <li class="navigation-item">
              <a class="navigation-link " href="/posts/">Blog</a>
            </li>
          
            <li class="navigation-item">
              <a class="navigation-link " href="/projects/">Projects</a>
            </li>
          
            <li class="navigation-item">
              <a class="navigation-link " href="/archive/">Archive</a>
            </li>
          
        
        
          
          
          
            
          
            
              
                <li class="navigation-item menu-separator">
                  <span>|</span>
                </li>
                
              
              <li class="navigation-item">
                <a href="/zh/">ZH</a>
              </li>
            
          
        
      </ul>
    
  </section>
</nav>


    <div class="content">
      
  <section class="container post">
    <article>
      <header>
        <div class="post-title">
          <h1 class="title">
            <a class="title-link" href="http://localhost:1313/posts/2023-7-24-kf/">
              Kalman Filter explained with derivation
            </a>
          </h1>
        </div>
        <div class="post-meta">
          <div class="date">
            <span class="posted-on">
              <i class="fa-solid fa-calendar" aria-hidden="true"></i>
              <time datetime="2023-07-24T00:00:00Z">
                July 24, 2023
              </time>
            </span>
            <span class="reading-time">
              <i class="fa-solid fa-clock" aria-hidden="true"></i>
              5-minute read
            </span>
          </div>
          
          
          
        </div>
      </header>

      <div class="post-content">
        
        <p>Kalman Filter (KF) is a recursive state estimator widely used in sensor/data fusion. It&rsquo;s an optimal Bayesian filter in linear Gaussian system, since it minimizes the estimated error covariance. It is also known as Gaussian Filter. KF is not applicable to discrete or hybrid state spaces. This blog helps understand Kalman Filter with simple examples and presents mathematical derivation from two different perspectives.</p>
<h1 id="modeling-robot-state-estimation">
  Modeling Robot State Estimation
  <a class="heading-link" href="#modeling-robot-state-estimation">
    <i class="fa-solid fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h1>
<details><summary>CLICK ME</summary>
<p>Now we want to estimate the state of a robot &lt;$p, v$&gt;, $p = [p_x, p_y, p_z]^T$ for position and $v = [v_x, v_y, v_z]^T$ for velocity. The state at time $k-1$ is $x_{k-1}=(p_{k-1},v_{k-1})$, we can predict the state at time $k$ if the accelaration $a_k$ is given:
$$
p_k = p_{k-1}+v_{k-1} \Delta t + \frac{1}{2}a_k (\Delta t)^2 \
v_k = v_{k-1}+a_k \Delta t
$$
In matrix form:
$$
x_k = Ax_{k-1}+Ba_k
$$
where $A$ is state transition matrix and $B$ is called control input matrix.
$$
A = \begin{bmatrix}1 &amp; \Delta t \ 0 &amp; 1 \end{bmatrix},
B = \begin{bmatrix} \frac{1}{2}(\Delta t)^2 \ \Delta t \end{bmatrix}
$$
This is the ideal case. In reality the robot is disturbed by so-called process noise, which models the uncertainty introduced by the state transition. Usually we assume the process noise $\omega_k$ follows a Gaussian distribution (it has to be Gaussian in KF) with zero mean $\omega_k \sim N(0,Q_k)$, $Q_k$ is the covariance matrix. Another observation of the state $z_k$ comes from sensor such as GPS, with measurement noise in Gaussian $v_k \sim N(0, R_k)$. State space model:
<div class="math">

$$
\begin{aligned}
&x_k = Ax_{k-1}+Ba_k+\omega_k \\
&z_k = Hx_k+v_k
\end{aligned}
$$

</div>

where $H_k$ is measurement matrix that map the state space into the observation space.</p>
</details>
<br>
<h1 id="mathematical-derivation">
  Mathematical Derivation
  <a class="heading-link" href="#mathematical-derivation">
    <i class="fa-solid fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h1>
<p>Kalman Filter is the optimal filter for Linear Gaussian system, a linear system with almost every variable be Gaussian distributed. KF can be derived in many ways, this section introduces 2 derivations to help better understand KF.</p>
<details><summary>Linear Optimal Estimation</summary>
<p>How can we guess the real value of $x$ if we observed $x=x_1$ from source 1 and $x=x_2$ from source 2? Theoratically we would never get the real value, but we can approaching the real value or minimize the error between estimated value and real value based on information from different sources under certain assumptions. One option for estimating $x$ is weighted sum of the $x_1$, $x_2$. Kalman Filter corrects the state prediction value according to observed value from sensors to approach the real value, which is similar to weighted sum:
<div class="math">

$$
\begin{aligned}
\tilde{x}_k = \tilde{x}^-_k + K(z_k-H\tilde{x}^-_k)
\end{aligned}
$$ 

</div>

where $\tilde{x}_k$ is the optimal estimate, $\tilde{x}^-_k$ is predicted state (source 1) and $z_k$ is the observation (source 2). $K$ is called Kalman gain. To determine the value of $K$:
<div class="math">

$$
\begin{aligned}
\tilde{x}_k = &\tilde{x}^-_k + K(Hx_k+v_k-H\tilde{x}^-_k) \\
= &\tilde{x}^-_k + KHx_k+Kv_k-KH\tilde{x}^-_k 
\end{aligned}
$$
subtract $x_k$ from both sides:
$$
\tilde{x}_k-x_k = \tilde{x}^-_k-x_k + KH(x_k-\tilde{x}^-_k)+Kv_k
$$

</div>

Simplify the equation:
<div class="math">

$$
e_k=x_k-\tilde{x}_k\\
e^-_k=x_k-\tilde{x}^-_k\\
e_k=(I-KH)e^-_k-Kv_k
$$

</div>

To ensure the estimate $\tilde{x}_k$ is optimal, we have to find the $K$ that minimize $||e_k||^2$, which is equivalent to minimize the trace of covariance matrix $tr(E[e_ke^T_k])$.
Set:
$$
P_k=E[e_ke^T_k],
P^-_k=E[e^-_ke^{-T}_k]
$$
Substitute $e_k$:
<div class="math">

$$
\begin{aligned}
P_k=&(I-KH)P^-_k(I-KH)^T+KRK^T\\
=&P^-_k -KHP^-_k-P^-_kH^TK^T+K(HP^-_kH^T+R)K^T
\end{aligned}
$$

</div>

To find the minimum of $tr(P_k)$, we can set the first derivative of $P_k$ with respect to $K$ to 0:
<div class="math">

$$
\frac{\partial P_k}{\partial K}=-2(P^-_kH^T)+2K(HP^-_kH^T+R)=0\\
K=P^-_kH^T(HP^-_kH^T+R)^{-1}
$$

</div>

Substituting $K^{-1}P^-_kH^T=HP^-_kH^T+R$ back into the expression of $P_k$:
$$
P_k=(I-K_kH)P^-_k
$$
We also need to obtain $P^-_k$. Note that noise are independent from the robot state:
<div class="math">

$$
\begin{aligned}
e^-_k=&x_k-\tilde{x}^-_k\\
=&Ax_{k-1}+Bu_k+\omega_k-A \tilde{x}_{k-1}-Bu_k\\
=&A(x_{k-1}-\tilde{x}_{k-1})+\omega_k\\
=&Ae_{k-1}+\omega_k
\end{aligned}\\
$$

</div>

<div class="math">

$$
\begin{aligned}
P^-_k=&E[e^-_ke^{-T}_k]\\
=&E[(Ae_{k-1}+\omega_k)(Ae_{k-1}+\omega_k)^T]\\
=&E[(Ae_{k-1})(Ae_{k-1})^T]+E[\omega_k(\omega_k)^T]\\
=&AP_{k-1}A^T+Q
\end{aligned}\\
$$

</div>

Finally, we can run the Kalman Filter: <br>
Step 1. Prediction
<div class="math">

$$
\begin{aligned}
\tilde{x}^-_k &= A\tilde{x}_{k-1}+Ba_k\\
P^-_k &= AP_{k-1}A^T+Q
\end{aligned}
$$

</div>
</p>
<p>Step 2. Measurement update
<div class="math">

$$
\begin{aligned}
K_k&=P^-_kH^T(HP^-_kH^T+R)^{-1}\\
\tilde{x}_k &= \tilde{x}^-_k + K(z_k-H\tilde{x}^-_k)\\
P_k&=(I-K_kH)P^-_k
\end{aligned}
$$

</div>

From the way we find the $K$ for optimal estimation (Least square error), Kalman Filter is the optimal Linear Filter for a Linear system if the noise are independent distributed with zero mean values and valid variances. However, if the system is not a Linear Gaussian, there are non-linear filters perform better than KF, which is another appealing topic. In another word, KF is the optimal estimater for Linear Gaussian System, see the proof below.</p>
</details>
<br>
<details><summary>Maximum A Posteriori Estimation</summary>
<p>KF is a Maximum A Posteriori estimator in Linear Gaussian space under Bayesian interpretation following the same 2-step scheme. The goal is finding the $x_k$ to maximize the posterior distribution:$\tilde{x_k}={argmax}_{x_k}\ p(x_k|z_k,a_k)$</p>
<p>Step 1. Prediction: predict
$\bar{bel}(x_k)=P(x_k|z_{k-1},a_k)$, the belief of $x_k$ after receiving the control input $a_k$ and before the measurement $z_k$ using the state model, given $bel(x_{t-1})=P(x_{k-1}|z_{k-1},a_{k-1})$, the belief of $x_{k-1}$ (A belief reflects the robot’s internal knowledge about the state of the environment):
<div class="math">

$$
\begin{aligned}
\bar{bel}(x_k)=&P(x_k|z_{k-1},a_k)\\
=&P(x_k|z_{k-1},a_k,a_{k-1})\\
=& \frac{P(x_k,z_{k-1}|a_k,a_{k-1})}{P(z_{k-1})}\\
=& \frac{\int p(x_k,x_{k-1},z_{k-1}|a_k,a_{k-1})dx_{k-1}}{P(z_{k-1})}\\
=& \frac{\int p(x_k|x_{k-1},z_{k-1},a_k,a_{k-1})p(x_{k-1}|z_{k-1},a_{k-1})P(z_{k-1})dx_{k-1}}{P(z_{k-1})}\\
=& \int p(x_k|x_{k-1},a_k)bel(x_{k-1})dx_{k-1}\\
\end{aligned}
$$

</div>
</p>
<p>Note that $x_k$ is only depend on $x_{k-1}$ and $a_k$ (Markov assumption) and $z_{k-1}$ is not dependent on $a_{k-1}$ if $x_{k-1}$ is known (see observation model).
<div class="math">

$$
\begin{aligned}
bel(x_{k-1}) &\sim N(x_{k-1};\mu_{k-1},\Sigma_{k-1})\\
p(x_k|x_{k-1},a_k) &\sim N(x_k;Ax_{k-1}+Ba_k,Q_k)
\end{aligned}
$$

</div>

The result of multiplying two Gaussian PDFs is in the form of a Gaussion PDF:
<div class="math">

$$
\begin{aligned}
\bar{bel}(x_{k}) &\sim N(x_k;\bar{\mu}_k,\bar{\Sigma}_k)\\
\bar{\mu}_k&=A\mu_{k-1}+Ba_k\\
\bar{\Sigma}_k &= A\Sigma_{k-1}A^T+Q_k
\end{aligned}
$$

</div>
</p>
<p>Step 2. Correction / Update: update the prediction using the observation at time $t$ to obtain the estimation $P(x_k|z_k,a_k)$
<div class="math">

$$
\begin{aligned}
bel(x_k) =&P(x_k|z_k,a_k)\\
=&P(x_k|z_k,z_{k-1},a_k)\\
=& \frac{P(x_k,z_k|z_{k-1},a_k)}{P(z_k|z_{k-1},a_k)}\\
=& \frac{P(z_k|x_k,z_{k-1},a_k)P(x_k|z_{k-1},a_k)}{P(z_k|z_{k-1},a_k)}\\
=& \frac{P(z_k|x_k)\bar{bel}(x_k)}{P(z_k|z_{k-1},a_k)}\\
=& \eta P(z_k|x_k)\bar{bel}(x_k)
\end{aligned}
$$

</div>

Assuming that $z_k$ is independent of previous observations $z_{1:k-1}$, conditional independent of $a_k$ given $x_k$. Normalization factor $\eta$ can be calculated:
<div class="math">

$$
\begin{aligned}
P(z_k|z_{k-1},a_k)=& \int p(z_k,x_k|z_{k-1},a_k)dx_k\\
=& \int p(z_k|x_k,z_{k-1},a_k)p(x_k|z_{k-1},a_k)dx_k \\
=& \int p(z_k|x_k)p(x_k|z_{k-1},a_k)dx_k\\
=& \int p(z_k|x_k)\bar{bel}(x_k)dx_k
\end{aligned}
$$

</div>

Likewise, Two Gaussian PDFs multiplication:
<div class="math">

$$
\begin{aligned}
p(z_t|x_t) &\sim N(z_k;Hx_k,R_k)\\
\bar{bel}(x_{k}) &\sim N(x_k;\bar{\mu}_k,\bar{\Sigma}_k)\\
bel(x_{k}) &=\eta \exp\{-J_k\}
\end{aligned}
$$

</div>

where
$J_k=\frac{1}{2}(z_k-Hx_k)^TR^{-1}_k(z_k-Hx_k)+\frac{1}{2}(x_k-\bar{\mu}_k)^T\bar{\Sigma}^{-1}_k(x_k-\bar{\mu}_k)
$.
$J_k$ is quadratic in $x_k$, which means $bel(x_t)$ is a Gaussian. The mean of $bel(x_k)$ is the minimum of this quadratic function, and $bel(x_k)$ reaches it&rsquo;s maximum. After complicated matrix operations and simplification, we will obtain:
<div class="math">

$$
\begin{aligned}
bel(x_{k}) &\sim N(x_k;\mu_k,\Sigma_k)\\
\mu_k&=\bar{\mu}_k+K_k(z_k-H\bar{\mu}_k)\\
\Sigma_k &= (I-K_kH)\bar{\Sigma}_k\\
K_k&=\bar{\Sigma}_kH^T(R_k+H\bar{\Sigma}_kH^T)^{-1}
\end{aligned}
$$

</div>

We can use MAP or just use bayesian inference, both works to find the same parameters, since the full Bayesian posterior is exactly Gaussian in a Linear Gaussian system.</p>
</details>
<br>

      </div>


      <footer>
        


        
        
        
        
        
        
        
      </footer>
    </article>

    
  </section>

    </div>

    <footer class="footer">
  <section class="container">
    ©
    
    2025
     Matt 
    ·
    
    Powered by <a href="https://gohugo.io/" target="_blank" rel="noopener">Hugo</a> & <a href="https://github.com/luizdepra/hugo-coder/" target="_blank" rel="noopener">Coder</a>.
    
  </section>
</footer>

  </main>

  

  
  
  <script src="/js/coder.js"></script>
  

  

  


  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  
</body>
</html>
