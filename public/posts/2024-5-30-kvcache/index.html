<!DOCTYPE html>
<html lang="en">

<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
  <title>
  LLMs Inference speed up EP1 - kv cache · Welcome
</title>
  <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="color-scheme" content="light dark">




<meta name="author" content="Matt">
<meta name="description" content="Large Language Models (LLMs) have revolutionized the field of natural language processing, enabling significant advancements in tasks such as language translation, text summarization, and sentiment analysis. However, despite their impressive capabilities, LLMs are not without limitations. One of the most significant challenges facing LLMs today is the problem of inference speed. Due to the sheer size and complexity of these models, the process of making predictions or extracting information from them can be computationally expensive and time-consuming. Several ways to speed up the LLMs without updating hardware: ">
<meta name="keywords" content="blog,developer,personal">



  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="LLMs Inference speed up EP1 - kv cache">
  <meta name="twitter:description" content="Large Language Models (LLMs) have revolutionized the field of natural language processing, enabling significant advancements in tasks such as language translation, text summarization, and sentiment analysis. However, despite their impressive capabilities, LLMs are not without limitations. One of the most significant challenges facing LLMs today is the problem of inference speed. Due to the sheer size and complexity of these models, the process of making predictions or extracting information from them can be computationally expensive and time-consuming. Several ways to speed up the LLMs without updating hardware:">

<meta property="og:url" content="http://localhost:1313/posts/2024-5-30-kvcache/">
  <meta property="og:site_name" content="Welcome">
  <meta property="og:title" content="LLMs Inference speed up EP1 - kv cache">
  <meta property="og:description" content="Large Language Models (LLMs) have revolutionized the field of natural language processing, enabling significant advancements in tasks such as language translation, text summarization, and sentiment analysis. However, despite their impressive capabilities, LLMs are not without limitations. One of the most significant challenges facing LLMs today is the problem of inference speed. Due to the sheer size and complexity of these models, the process of making predictions or extracting information from them can be computationally expensive and time-consuming. Several ways to speed up the LLMs without updating hardware:">
  <meta property="og:locale" content="en">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-05-31T00:00:00+00:00">
    <meta property="article:modified_time" content="2024-05-31T00:00:00+00:00">




<link rel="canonical" href="http://localhost:1313/posts/2024-5-30-kvcache/">



<link rel="preload" href="/fonts/fa-brands-400.woff2" as="font" type="font/woff2" crossorigin>
<link rel="preload" href="/fonts/fa-regular-400.woff2" as="font" type="font/woff2" crossorigin>
<link rel="preload" href="/fonts/fa-solid-900.woff2" as="font" type="font/woff2" crossorigin>


  
  
  <link rel="stylesheet" href="/css/coder.css" media="screen">






  
    
    
    <link rel="stylesheet" href="/css/coder-dark.css" media="screen">
  



 




<link rel="icon" type="image/svg+xml" href="/images/favicon.svg" sizes="any">
<link rel="icon" type="image/png" href="/images/favicon-32x32.png" sizes="32x32">
<link rel="icon" type="image/png" href="/images/favicon-16x16.png" sizes="16x16">

<link rel="apple-touch-icon" href="/images/apple-touch-icon.png">
<link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png">

<link rel="manifest" href="/site.webmanifest">
<link rel="mask-icon" href="/images/safari-pinned-tab.svg" color="#5bbad5">










<script>
MathJax = {
  tex: {
    inlineMath: [['$', '$'], ['\\(', '\\)']],
    displayMath: [['$$', '$$'], ['\\[', '\\]']],
    processEscapes: true,
    processEnvironments: true,
    packages: {'[+]': ['ams', 'noerrors']}
  },
  svg: {
    fontCache: 'global'
  },
  options: {
    renderActions: {
       
      find_script_mathtex: [10, function (doc) {
        for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
          const display = !!node.type.match(/; *mode=display/);
          const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
          const text = document.createTextNode('');
          node.parentNode.replaceChild(text, node);
          math.start = {node: text, delim: '', n: 0};
          math.end = {node: text, delim: '', n: 0};
          doc.math.push(math);
        }
      }, '']
    }
  }
};
</script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>


</head>






<body class="preload-transitions colorscheme-auto">
  
<div class="float-container">
    <a id="dark-mode-toggle" class="colorscheme-toggle">
        <i class="fa-solid fa-adjust fa-fw" aria-hidden="true"></i>
    </a>
</div>


  <main class="wrapper">
    <nav class="navigation">
  <section class="container">
    
    <a class="navigation-title" href="http://localhost:1313/">
      Welcome
    </a>
    
    
      <input type="checkbox" id="menu-toggle" />
      <label class="menu-button float-right" for="menu-toggle">
        <i class="fa-solid fa-bars fa-fw" aria-hidden="true"></i>
      </label>
      <ul class="navigation-list">
        
          
            <li class="navigation-item">
              <a class="navigation-link " href="/about/">About</a>
            </li>
          
            <li class="navigation-item">
              <a class="navigation-link " href="/posts/">Blog</a>
            </li>
          
            <li class="navigation-item">
              <a class="navigation-link " href="/projects/">Projects</a>
            </li>
          
            <li class="navigation-item">
              <a class="navigation-link " href="/archive/">Archive</a>
            </li>
          
        
        
          
          
          
            
          
            
              
                <li class="navigation-item menu-separator">
                  <span>|</span>
                </li>
                
              
              <li class="navigation-item">
                <a href="/zh/">ZH</a>
              </li>
            
          
        
      </ul>
    
  </section>
</nav>


    <div class="content">
      
  <section class="container post">
    <article>
      <header>
        <div class="post-title">
          <h1 class="title">
            <a class="title-link" href="http://localhost:1313/posts/2024-5-30-kvcache/">
              LLMs Inference speed up EP1 - kv cache
            </a>
          </h1>
        </div>
        <div class="post-meta">
          <div class="date">
            <span class="posted-on">
              <i class="fa-solid fa-calendar" aria-hidden="true"></i>
              <time datetime="2024-05-31T00:00:00Z">
                May 31, 2024
              </time>
            </span>
            <span class="reading-time">
              <i class="fa-solid fa-clock" aria-hidden="true"></i>
              6-minute read
            </span>
          </div>
          
          
          
        </div>
      </header>

      <div class="post-content">
        
        <p>Large Language Models (LLMs) have revolutionized the field of natural language processing, enabling significant advancements in tasks such as language translation, text summarization, and sentiment analysis. However, despite their impressive capabilities, LLMs are not without limitations. One of the most significant challenges facing LLMs today is the problem of inference speed. Due to the sheer size and complexity of these models, the process of making predictions or extracting information from them can be computationally expensive and time-consuming. Several ways to speed up the LLMs without updating hardware: <br></p>
<p>a. Parallelism: Transformers are designed to work in parallel (Self-attention). One can take advantage of parallel processing on multi-core CPUs or multi-GPU systems.</p>
<p>b. Quantization: Quantizing the weights and activations of neural networks can help reduce the computational requirements for inference, meanwhile save memory usage.</p>
<p>c. Pruning: Pruning techniques can be used to remove redundant connections in neural networks, thereby reducing their size and computational requirements.</p>
<p>d. Knowledge Distillation: This technique involves transferring knowledge from a larger pre-trained model (the teacher) to a smaller model (the student). By leveraging the knowledge learned by the teacher, the student model can achieve better performance with less computational resources.</p>
<p>There are simplier ways, from engineering perspective, to optimizae LLM inference without changing the model architecture: 1. Reduce duplicate calculations (fewer FLOPs), 2. Reduce data transfer time (flash attention)</p>
<h1 id="kv-cache">
  KV Cache
  <a class="heading-link" href="#kv-cache">
    <i class="fa-solid fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h1>
<p>During inference, an LLM produces its output token by token, also known as autoregressive decoding. Each generated token depends on all previous tokens, including those in the prompt and any previously generated output tokens.</p>
<p>Therefore, the Key and Value of previous tokens are involved in the computation for the next token. The long queue of tokens can cause computation bottleneck in the self-attention stage, if we don&rsquo;t use the K, V calculated before. To better illustrate this issue, here&rsquo;s an example shows how self attention works.</p>
<p>The attention score of the first token is obtained by:
$$
Att_1(Q,K,V)=softmax(\frac{Q_1K^T_1}{\sqrt{d}})\vec{V_1}
$$
where $K=W_k x$, $Q=W_q x$, $V=W_v x$, $x$ is embedding vector, $d$ is the dimension of embedding vectors, we can overlook it in this discussion. When generating the second token, attention matrix:
<div class="math">

$$
\begin{aligned}
Att(Q,K,V) &=softmax(
    \begin{bmatrix}
    Q_1K^T_1       & - \infty \\
    Q_2K^T_1       & Q_2K^T_2 
    \end{bmatrix}
)
\begin{bmatrix}
    \vec{V_1} \\
    \vec{V_2}      
\end{bmatrix} \\
&=
\begin{bmatrix}
    softmax(Q_1K^T_1)\vec{V_1} \\
    softmax(Q_2K^T_1)\vec{V_1} + softmax(Q_2K^T_2)\vec{V_2} 
\end{bmatrix}
\end{aligned}
$$

</div>

Attention score for the second token is:
$$
Att_2(Q,K,V)=softmax(Q_2K^T_1)\vec{V_1} + softmax(Q_2K^T_2)\vec{V_2}
$$
Likewise, we can get the attention score for the thrid token:
$$
Att_3(Q,K,V)=softmax(Q_3K^T_1)\vec{V_1} + softmax(Q_3K^T_2)\vec{V_2} + softmax(Q_3K^T_3)\vec{V_3}
$$
We can see Key and Value used from last iteration are reused to generate the next token, thus store K, V to reduce computation.</p>
<p><img src="/images/img/LLM/kvcache.gif" alt="kvcache"></p>
<p>Nice gif from <a href="https://medium.com/@joaolages/kv-caching-explained-276520203249"  class="external-link" target="_blank" rel="noopener">joao lages</a>.</p>
<details><summary>Memory is eaten up?</summary>
<p>KV cache can be very large, sometimes up to several GB. Let&rsquo;s see its size if data is stored in fp16 (2 bytes) for a single batch:
$$
kv_size = 2<em>2</em>d*n_layers * max_context_length
$$
Note that for Grouped-query Attention (GQA), multiple heads shared the same Key and Value, which could reduce the kv cache size.</p>
<br>
<img src='/images/img/LLM/GQA.png' alt="GQA">
<br>
<p>There are research ongoing showing that quantized (with some tricks) KV cache also works as well, things aren&rsquo;t that bad. Overall, more memory consumption for less computation and faster inference, it&rsquo;s a fair trade-off.</p>
</details>
<br>
<h1 id="pagedattention">
  PagedAttention
  <a class="heading-link" href="#pagedattention">
    <i class="fa-solid fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h1>
<p>In the begining, KV cache is simply saved in VRAM, which requires large contiguous memory allocation. PagedAttention finds that there&rsquo;s a considerable amount of memory being wasted in the KV cache because of excessive reservation. Instead of directly allocate the maximum amount of memory to fill the model context length, no matter how long the real prompt and generation would be, Pagedattention dynamicly allocates memory and save KV cache in non-contiguous blocks:</p>
<br>
<img src='/images/img/LLM/pagedattention.png' alt="pagedattention">
<br>
<p>Pagedattention allows more sophisticated management, for instance, when multiple inference requests share the same large initial system prompt, it&rsquo;s wise to save key and value vectors for the initial prompt once and share among requests.
Now you can tell why the Pagedattention is so popular: it increases model throughput without any architecture modification, functions as a cache management layer that can be seamlessly integrated with various LLMs, especially useful in scenarios where LLMs handle large batches of prompts.</p>
<p>Further thoughts on memory management, pre-allocating memory is not bad as long as the allocated blocks are filled. vAttension pre-reserves virtual memory (continuous) to save the time on looking up block tables in Pagedattention, and allocates physical memory in runtime &ndash; one page at a time and only when a request has used all of its previously allocated physical memory page.</p>
<h1 id="flash-attention">
  Flash Attention
  <a class="heading-link" href="#flash-attention">
    <i class="fa-solid fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h1>
<p>Authors of Flash attention found that attention operation is memory-bound caused by tensor transfer within a GPU, which means instead of targeting at matrix multiplication and reducing FLOPs, an IO-aware attention is the key to acceleration and overcome speed bottleneck. To better understand the intention of flash attention, here&rsquo;s the structure of GPU memory, standard attention implementation and its time cost. <br></p>
<p><img src="/images/img/LLM/att.png" alt="standard attention and gpu memory"></p>
<p>To perform a standard attention operation in GPU, tensors generated in intermediate steps are transfered between SRAM and HBM multiple times, results in much more time cost than matrix multiplication.
In comparison, flash attention works in the following way:<br></p>
<p><img src="/images/img/LLM/faa.png" alt="fa"></p>
<p>The core is matrix split in softmax (also known as tiling). Recall the softmax computation of vector $x \in R^B$:
<div class="math">

$$
\begin{aligned}
&m(x):=\max_{i} (x_i)\\
&f(x):=[e^{x_1-m(x)},...,e^{x_B-m(x)}]\\
&l(x):=\sum_if(x)_i\\
&softmax:=\frac{f(x)}{l(x)}
\end{aligned}
$$

</div>

For vector $x^{(1)},x^{(2)} \in R^B$, the softmax of concatenated vector $x=[x^{(1)},x^{(2)}] \in R^{2B}$ is computed as:
<div class="math">

$$
\begin{aligned}
&m(x)=m([x^{(1)},x^{(2)}])=\max(m(x^{(1)}),m(x^{(2)}))\\
&f(x)=[e^{m(x^{(1)})-m(x)}f(x^{(1)}),e^{m(x^{(2)})-m(x)}f(x^{(2)})]\\
&l(x)=e^{m(x^{(1)})-m(x)}l(x^{(1)})+e^{m(x^{(2)})-m(x)}l(x^{(2)})\\
&softmax=\frac{f(x)}{l(x)}
\end{aligned}
$$

</div>

In this way, we can split $Q,K,V$ into blocks and incrementally perform the softmax reduction, compute softmax one block at a time. Flash attention does not store intermediate values ($S,P \in R^{N \times N}$ in algorithm 1) to compute gradients w.r.t $Q,K,V$ during backward pass, instead $S.P$ are recomputed by storing the output O and the softmax normalization statistics $(m,l)$. Recomputation not only reduces the required memory, but also speeds up the backward pass due to the reduction of HBM accesses. That&rsquo;s why in algorithm 1 the block sizes are set to $\frac{M}{4d}$: blocks of $Q,K,V,O$ are loaded to SRAM and $m,l$ are kept in registers during computation. Another graph to illustrate:<br></p>
<p><img src="/images/img/LLM/fa.png" alt="flash"></p>
<p>The final road block when trying to understand algorithm 1 may lies in step 12. $O_i$ is intermediate computed attention result used during iteration. Since $S,P$ are not stored in memory, $\text{diag} (l_i) e^{m_i-m_i^{new}} O_i$ is to recover $PV$ from last iteration. Diagonal matrix here is a way to operate mul/div on matrix row by row, and the exponential part simply updates the new max value, these small mathematical tricks may be clearer after you verify it yourself.
More details and proofs are provided in the paper.</p>

      </div>


      <footer>
        


        
        
        
        
        
        
        
      </footer>
    </article>

    
  </section>

    </div>

    <footer class="footer">
  <section class="container">
    ©
    
    2025
     Matt 
    ·
    
    Powered by <a href="https://gohugo.io/" target="_blank" rel="noopener">Hugo</a> & <a href="https://github.com/luizdepra/hugo-coder/" target="_blank" rel="noopener">Coder</a>.
    
  </section>
</footer>

  </main>

  

  
  
  <script src="/js/coder.js"></script>
  

  

  


  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  
</body>
</html>
