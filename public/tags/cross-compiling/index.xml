<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Cross Compiling on Welcome</title>
    <link>http://localhost:1313/tags/cross-compiling/</link>
    <description>Recent content in Cross Compiling on Welcome</description>
    <generator>Hugo</generator>
    <language>en</language>
    <atom:link href="http://localhost:1313/tags/cross-compiling/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Deploy LLMs on a phone with arm chips</title>
      <link>http://localhost:1313/posts/2024-6-13-chatbot3/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/2024-6-13-chatbot3/</guid>
      <description>&lt;p&gt;In this blog, I&amp;rsquo;m going to share how I deployed LLMs (1~3b) and ASR models (whisper tiny &amp;amp; base) on my old smart phone, Xiaomi 8 (dipper). It was released in 2018, with a broken screen and depleted battery after several years of heavy usage, it still offers acceptable speed and fluency running daily light-weight apps. So I wonder, if I deploy modern machine models, such as LLMs, on the phone, and can I run the large models smoothly. Let&amp;rsquo;s see.&lt;br&gt;&#xA;The hardware configuration: 8-cores CPU with 4 big cores (Cortex-A75) and 4 small cores (Cortex-A55), 64 GB storage and 6 GB RAM. It has a integrated GPU, which I didn&amp;rsquo;t find a way to utilize during model inference, so the computation is totally counted on CPU in this trial. To better leverage the power of this CPU, I uninstall the android system (MIUI 12) and port a Ubuntu touch on the phone. Basically it&amp;rsquo;s a linux OS but using the underlying andriod framework to control hardwares, and it gives me a much longer battery life compared to android, also more convenience since I am a rookie in android development. Models are quantized versions from &lt;a href=&#34;https://github.com/ggerganov/llama.cpp&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;llamacpp&lt;/a&gt; and &lt;a href=&#34;https://github.com/ggerganov/whisper.cpp&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;whispercpp&lt;/a&gt;.&lt;br&gt;&#xA;To get rid of the cumbersome work build an app with GUI, I run models in command line using the terminal app originally from Ubuntu touch, which can be regarded as the same terminal on Ubuntu desktop in this case. All I need to do is to compile the c++ code into an executable file that runs on my phone. Since the architecture of my laptop cpu is x86, the version of glibc, libstdc++ are different from the libs on the phone, I could either compile on the phone directly or cross compile on my PC with a specific toolchain. I kept all the heavy work on PC and built my own toolchain using &lt;a href=&#34;https://crosstool-ng.github.io/docs/&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;crosstool-NG&lt;/a&gt;, which is targeted at building toolchains. &lt;br&gt;&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
