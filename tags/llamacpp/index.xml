<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Llamacpp on Welcome</title>
    <link>http://localhost:1313/tags/llamacpp/</link>
    <description>Recent content in Llamacpp on Welcome</description>
    <generator>Hugo</generator>
    <language>en</language>
    <lastBuildDate>Wed, 03 Apr 2024 00:00:00 +0000</lastBuildDate>
    <atom:link href="http://localhost:1313/tags/llamacpp/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Customize a local chatbot in streamlit</title>
      <link>http://localhost:1313/posts/2024-4-4-chatbot2/</link>
      <pubDate>Wed, 03 Apr 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/2024-4-4-chatbot2/</guid>
      <description>&lt;p&gt;Large Language Models (LLMs) are the brains of various chatbots, and luckily we all have access to some brilliant cheap or even free LLMs now, thanks to open source community. It is possible to run LLMs on a PC and keep everything local. This blog presents my solution to building a chatbot running on my PC, with a totally local file storage system and a costumized graphical user interface.&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;http://localhost:1313/images/img/chatbot/chatbot.gif&#34; alt=&#34;My local chatbot&#34;&gt;&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
