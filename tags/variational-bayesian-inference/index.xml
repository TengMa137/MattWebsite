<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Variational Bayesian Inference on Welcome</title>
    <link>http://localhost:1313/tags/variational-bayesian-inference/</link>
    <description>Recent content in Variational Bayesian Inference on Welcome</description>
    <generator>Hugo</generator>
    <language>en</language>
    <lastBuildDate>Sun, 31 Dec 2023 00:00:00 +0000</lastBuildDate>
    <atom:link href="http://localhost:1313/tags/variational-bayesian-inference/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Variational Autoencoder explained</title>
      <link>http://localhost:1313/posts/2023-12-31-vae/</link>
      <pubDate>Sun, 31 Dec 2023 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/2023-12-31-vae/</guid>
      <description>&lt;p&gt;Variational Autoencoders (VAEs) are a powerful class of generative models used in unsupervised learning, combining the strengths of both neural networks and probabilistic models. They aim to approximate an underlying latent space that represents the distribution of data. To achieve this, VAEs learn to encode high-dimensional input data into a lower dimensional set of latent variables using a variational inference process. By doing so, they can reconstruct the original input while also enabling manipulation and generation of new samples within the learned distribution. This is particularly useful for applications like image synthesis, where the ability to encode complex data and then decode it as new variations of that data is crucial.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
